{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn, optim\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize  # Ensure NLTK is installed\n",
    "from sklearn.metrics import accuracy_score\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL VARIABLES ###\n",
    "SEED = 28\n",
    "SPECIAL_TOKENS = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNKNOWN>': 3}\n",
    "MAX_LEN = 17\n",
    "### END OF GLOBAL VARIABLES ###\n",
    "\n",
    "# set random seed\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)  # Seed for CPU computations\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)  # Seed for GPU computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1992, 2)\n",
      "(2371921, 2)\n",
      "(597, 2)\n",
      "(23719, 2)\n",
      "Using 95th percentile for MAX_LEN: 17\n"
     ]
    }
   ],
   "source": [
    "# Load in data\n",
    "test_data = pd.read_csv('data_jesc/test', sep='\\t')\n",
    "train_data = pd.read_csv('data_jesc/train', sep='\\t')\n",
    "print(test_data.shape)\n",
    "print(train_data.shape)\n",
    "\n",
    "# Shuffle and take 50% of the data\n",
    "train_data = train_data.sample(frac=1, random_state=42).head(int(train_data.shape[0] * 0.01))\n",
    "test_data = test_data.sample(frac=1, random_state=42).head(int(test_data.shape[0] * 0.30))\n",
    "print(test_data.shape)\n",
    "print(train_data.shape)\n",
    "\n",
    "# Adjust columns\n",
    "test_data.columns = ['ENG', 'JPN']\n",
    "train_data.columns = ['ENG', 'JPN']\n",
    "\n",
    "# Tokenize English and Japanese sentences\n",
    "test_data['ENG'] = test_data['ENG'].apply(word_tokenize)\n",
    "test_data['JPN'] = test_data['JPN'].apply(word_tokenize)\n",
    "train_data['ENG'] = train_data['ENG'].apply(word_tokenize)\n",
    "train_data['JPN'] = train_data['JPN'].apply(word_tokenize)\n",
    "\n",
    "# Analyze sentence lengths\n",
    "all_lengths = pd.concat([\n",
    "    train_data['ENG'].apply(len),\n",
    "    train_data['JPN'].apply(len),\n",
    "    test_data['ENG'].apply(len),\n",
    "    test_data['JPN'].apply(len)\n",
    "])\n",
    "\n",
    "# Choose a percentile for MAX_LEN (e.g., 95th percentile)\n",
    "MAX_LEN = int(all_lengths.quantile(0.95))\n",
    "print(\"Using 95th percentile for MAX_LEN:\", MAX_LEN)\n",
    "\n",
    "SPECIAL_TOKENS = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNKNOWN>': 3}\n",
    "\n",
    "# Adjust build_vocab to limit vocabulary size\n",
    "def build_vocab(tokenized_data, special_tokens, max_vocab_size=5000):\n",
    "    tokenized_data = tokenized_data.explode()  # Flatten the DataFrame column\n",
    "    vocab_counter = Counter(tokenized_data)\n",
    "    most_common = vocab_counter.most_common(max_vocab_size)  # Keep only the top `max_vocab_size` words\n",
    "    vocab = {word: idx + len(special_tokens) for idx, (word, _) in enumerate(most_common)}\n",
    "    vocab.update(special_tokens)\n",
    "    return vocab\n",
    "\n",
    "eng_vocab = build_vocab(train_data['ENG'], SPECIAL_TOKENS, max_vocab_size=5000)\n",
    "jpn_vocab = build_vocab(train_data['JPN'], SPECIAL_TOKENS, max_vocab_size=5000)\n",
    "\n",
    "# Token-to-index conversion\n",
    "def safe_tokens_to_indices(tokens, vocab, sos_eos=True):\n",
    "    indices = [vocab.get(token, vocab['<UNKNOWN>']) for token in tokens]\n",
    "    if sos_eos:\n",
    "        indices = [vocab['<START>']] + indices + [vocab['<END>']]\n",
    "    return indices\n",
    "\n",
    "def pad_sequence(sequence, max_len=MAX_LEN, pad_value=0):\n",
    "    return sequence[:max_len] + [pad_value] * max(0, max_len - len(sequence))\n",
    "\n",
    "# Preprocess data\n",
    "train_data['ENG'] = train_data['ENG'].apply(lambda x: pad_sequence(safe_tokens_to_indices(x, eng_vocab)))\n",
    "train_data['JPN'] = train_data['JPN'].apply(lambda x: pad_sequence(safe_tokens_to_indices(x, jpn_vocab)))\n",
    "test_data['ENG'] = test_data['ENG'].apply(lambda x: pad_sequence(safe_tokens_to_indices(x, eng_vocab)))\n",
    "test_data['JPN'] = test_data['JPN'].apply(lambda x: pad_sequence(safe_tokens_to_indices(x, jpn_vocab)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, tgt_data):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_data[idx], dtype=torch.long), torch.tensor(self.tgt_data[idx], dtype=torch.long)\n",
    "\n",
    "### MODELS ###\n",
    "# LSTM Seq2Seq Model\n",
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.dropout = dropout\n",
    "        self.embedding_src = nn.Embedding(input_dim, embed_dim)\n",
    "        self.embedding_tgt = nn.Embedding(output_dim, embed_dim)\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_dim, n_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.embedding_src(src)\n",
    "        _, (hidden, cell) = self.encoder(src_embedded)\n",
    "        tgt_embedded = self.embedding_tgt(tgt)\n",
    "        outputs, _ = self.decoder(tgt_embedded, (hidden, cell))\n",
    "        return self.fc_out(outputs)\n",
    "\n",
    "# CNN Seq2Seq Model\n",
    "class CNNSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim, kernel_size, num_channels):\n",
    "        super(CNNSeq2Seq, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.num_channels = num_channels\n",
    "        self.embed_dim = embed_dim\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.embedding_src = nn.Embedding(input_dim, embed_dim)\n",
    "        self.embedding_tgt = nn.Embedding(output_dim, embed_dim)\n",
    "        self.encoder = nn.Conv1d(embed_dim, num_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.decoder = nn.Conv1d(num_channels + embed_dim, num_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.fc_out = nn.Linear(num_channels, output_dim)\n",
    "    def forward(self, src, tgt):\n",
    "        # Shape: (batch, seq_len, embed_dim) -> (batch, embed_dim, seq_len)\n",
    "        src_embedded = self.embedding_src(src).permute(0, 2, 1)  \n",
    "        tgt_embedded = self.embedding_tgt(tgt).permute(0, 2, 1)  \n",
    "        # Encoder outputs\n",
    "        encoder_outputs = self.encoder(src_embedded)  # Shape: (batch, num_channels, seq_len)\n",
    "        # Ensure tgt_embedded matches encoder_outputs in sequence length\n",
    "        tgt_embedded = tgt_embedded[:, :, :encoder_outputs.size(2)]  # Truncate if necessary\n",
    "        if tgt_embedded.size(2) < encoder_outputs.size(2):  # Pad if necessary\n",
    "            pad_size = encoder_outputs.size(2) - tgt_embedded.size(2)\n",
    "            tgt_embedded = torch.nn.functional.pad(tgt_embedded, (0, pad_size))\n",
    "        # Concatenate along the channel dimension\n",
    "        decoder_inputs = torch.cat((encoder_outputs, tgt_embedded), dim=1)  # Shape: (batch, num_channels + embed_dim, seq_len)\n",
    "        decoder_outputs = self.decoder(decoder_inputs)  # Shape: (batch, num_channels, seq_len)\n",
    "        # Final output: (batch, seq_len, output_dim)\n",
    "        return self.fc_out(decoder_outputs.permute(0, 2, 1))\n",
    "\n",
    "# Transformer Seq2Seq Model\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim, n_heads, num_layers, dropout):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "        self.embedding_src = nn.Embedding(input_dim, embed_dim)\n",
    "        self.embedding_tgt = nn.Embedding(output_dim, embed_dim)\n",
    "        self.transformer = nn.Transformer(embed_dim, n_heads, num_layers, num_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.n_heads = n_heads\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.embed_dim = embed_dim\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.embedding_src(src).permute(1, 0, 2)  # Shape: (seq_len, batch, embed_dim)\n",
    "        tgt_embedded = self.embedding_tgt(tgt).permute(1, 0, 2)  # Shape: (seq_len, batch, embed_dim)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_embedded.size(0)).to(src.device)\n",
    "        output = self.transformer(src_embedded, tgt_embedded, tgt_mask=tgt_mask)\n",
    "        return self.fc_out(output.permute(1, 0, 2))  # Shape: (batch, seq_len, output_dim)\n",
    "\n",
    "### END OF MODELS ###\n",
    "\n",
    "# train and test\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1]) # predict exclude last token\n",
    "        # align dimensions for loss calculation\n",
    "        if output.size(1) != tgt[:, 1:].size(1):\n",
    "            min_seq_len = min(output.size(1), tgt[:, 1:].size(1))\n",
    "            output = output[:, :min_seq_len, :]\n",
    "            tgt = tgt[:, :min_seq_len + 1]\n",
    "        # get loss\n",
    "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))  # Exclude first token from target\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Calculate acc\n",
    "        preds = output.argmax(dim=-1)  # Get the predicted token indices\n",
    "        correct = (preds == tgt[:, 1:]).float()  # Compare predictions to target tokens (shifted by 1)\n",
    "        total_correct += correct.sum().item()\n",
    "        total_samples += tgt[:, 1:].numel()  # Total number of target tokens\n",
    "        total_loss += loss.item()\n",
    "    # Return avg acc and loss\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "    return avg_loss, avg_accuracy\n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    \"\"\"\n",
    "    Tests model performance based on test_loader\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in test_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            output = model(src, tgt[:, :-1])  # Predict target sequence excluding the last token\n",
    "            \n",
    "            # Align sequence lengths between output and target\n",
    "            if output.size(1) != tgt[:, 1:].size(1):\n",
    "                min_seq_len = min(output.size(1), tgt[:, 1:].size(1))\n",
    "                output = output[:, :min_seq_len, :]\n",
    "                tgt = tgt[:, :min_seq_len + 1]\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "\n",
    "            # Calculate accuracy\n",
    "            preds = output.argmax(dim=-1)  # Get the predicted token indices\n",
    "            correct = (preds == tgt[:, 1:]).float()  # Compare predictions to target tokens (shifted by 1)\n",
    "            total_correct += correct.sum().item()\n",
    "            total_samples += tgt[:, 1:].numel()  # Total number of target tokens\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    avg_accuracy = total_correct / total_samples\n",
    "    return avg_loss, avg_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class GRUSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        \"\"\"\n",
    "        GRU-based Seq2Seq model for machine translation.\n",
    "\n",
    "        Args:\n",
    "            input_dim (int): Size of the input vocabulary.\n",
    "            output_dim (int): Size of the output vocabulary.\n",
    "            embed_dim (int): Embedding dimension.\n",
    "            hidden_dim (int): Hidden state dimension.\n",
    "            n_layers (int): Number of GRU layers.\n",
    "            dropout (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super(GRUSeq2Seq, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim \n",
    "        self.embed_dim = embed_dim \n",
    "        self.hidden_dim = hidden_dim \n",
    "        self.n_layers = n_layers \n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding_src = nn.Embedding(input_dim, embed_dim)\n",
    "        self.embedding_tgt = nn.Embedding(output_dim, embed_dim)\n",
    "        self.encoder = nn.GRU(embed_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.GRU(embed_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        Forward pass for GRU-based Seq2Seq model.\n",
    "        \"\"\"\n",
    "        src_embedded = self.dropout(self.embedding_src(src))  # (batch_size, src_seq_len, embed_dim)\n",
    "        tgt_embedded = self.dropout(self.embedding_tgt(tgt))  # (batch_size, tgt_seq_len, embed_dim)\n",
    "        _, hidden = self.encoder(src_embedded)  # hidden: (n_layers, batch_size, hidden_dim)\n",
    "        decoder_outputs, _ = self.decoder(tgt_embedded, hidden)  # (batch_size, tgt_seq_len, hidden_dim)\n",
    "        outputs = self.fc_out(decoder_outputs)  # (batch_size, tgt_seq_len, output_dim)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/penelopeking/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Transformer_Seq2Seq...\n",
      "Epoch 1: Train Loss = 5.6947, Train Accuracy = 0.0591\n",
      "Epoch 11: Train Loss = 5.6223, Train Accuracy = 0.0598\n",
      "Epoch 21: Train Loss = 5.6219, Train Accuracy = 0.0600\n",
      "Epoch 31: Train Loss = 5.6233, Train Accuracy = 0.0599\n"
     ]
    }
   ],
   "source": [
    "### TRAINING AND TESTING ###\n",
    "train_dataset = TranslationDataset(train_data['ENG'].tolist(), train_data['JPN'].tolist())\n",
    "test_dataset = TranslationDataset(test_data['ENG'].tolist(), test_data['JPN'].tolist())\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "models = {\n",
    "    'Transformer_Seq2Seq': TransformerSeq2Seq(len(eng_vocab), len(jpn_vocab), 15, 3, 3, 0.2),\n",
    "    'CNN_Seq2Seq': CNNSeq2Seq(len(eng_vocab), len(jpn_vocab), 24, kernel_size=3, num_channels=5),\n",
    "    'LSTM_Seq2Seq': LSTMSeq2Seq(len(eng_vocab), len(jpn_vocab), 24, 15, 3, 0.2),\n",
    "    'GRU_Seq2Seq': GRUSeq2Seq(len(eng_vocab), len(jpn_vocab), 24, 4, 3, 0.2),\n",
    "}\n",
    "\n",
    "models2 = {\n",
    "    'CNN_Seq2Seq': CNNSeq2Seq(len(eng_vocab), len(jpn_vocab), 24, kernel_size=4, num_channels=5),\n",
    "}\n",
    "\n",
    "results = {}\n",
    "EPOCHS = 50\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 0.01, weight_decay = 1e-4)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=SPECIAL_TOKENS['<PAD>'])\n",
    "    print(f\"Training {name}...\")\n",
    "    \n",
    "    for epoch in range(EPOCHS):  # Adjust number of epochs\n",
    "        train_loss, train_accuracy = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch + 1}: Train Loss = {train_loss:.4f}, Train Accuracy = {train_accuracy:.4f}\")\n",
    "    test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)\n",
    "    print(f\"{name} Test Loss = {test_loss:.4f}, Test Accuracy = {test_accuracy:.4f}\")\n",
    "    results[name] = (test_loss, test_accuracy)\n",
    "\n",
    "# Print comparison results\n",
    "print(\"\\nModel Comparison:\")\n",
    "for model_name, (loss, accuracy) in results.items():\n",
    "    print(f\"{model_namgit ae}: Test Loss = {loss:.4f}, Test Accuracy = {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabularies saved successfully!\n",
      "Vocabularies loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_vocab(vocab, path):\n",
    "    \"\"\"\n",
    "    Save a vocabulary dictionary to a JSON file.\n",
    "\n",
    "    Args:\n",
    "        vocab (dict): The vocabulary dictionary to save.\n",
    "        path (str): The file path to save the vocabulary.\n",
    "    \"\"\"\n",
    "    with open(path, 'w') as f:\n",
    "        json.dump(vocab, f, ensure_ascii=False, indent=4)  # Use ensure_ascii=False for non-ASCII tokens\n",
    "\n",
    "def load_vocab(path):\n",
    "    \"\"\"\n",
    "    Load a vocabulary dictionary from a JSON file.\n",
    "\n",
    "    Args:\n",
    "        path (str): The file path to load the vocabulary from.\n",
    "\n",
    "    Returns:\n",
    "        dict: The loaded vocabulary dictionary.\n",
    "    \"\"\"\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "# Save English and Japanese vocabularies\n",
    "save_vocab(eng_vocab, 'saved/eng_vocab.json')\n",
    "save_vocab(jpn_vocab, 'saved/jpn_vocab.json')\n",
    "\n",
    "print(\"Vocabularies saved successfully!\")\n",
    "\n",
    "# Load English and Japanese vocabularies\n",
    "eng_vocab = load_vocab('saved/eng_vocab.json')\n",
    "jpn_vocab = load_vocab('saved/jpn_vocab.json')\n",
    "\n",
    "print(\"Vocabularies loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Transformer_Seq2Seq to saved/Transformer_Seq2Seq_params.json and saved/Transformer_Seq2Seq_weights.pt.\n",
      "Saved CNN_Seq2Seq to saved/CNN_Seq2Seq_params.json and saved/CNN_Seq2Seq_weights.pt.\n",
      "Saved LSTM_Seq2Seq to saved/LSTM_Seq2Seq_params.json and saved/LSTM_Seq2Seq_weights.pt.\n",
      "Loaded LSTM_Seq2Seq (LSTM) from saved/LSTM_Seq2Seq_weights.pt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/penelopeking/anaconda3/envs/dsc80/lib/python3.8/site-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "### SAVING AND LOADING MODELS ###\n",
    "def save_model_and_params(models, save_dir):\n",
    "    \"\"\"\n",
    "    Save models and their parameters.\n",
    "    Args:\n",
    "        models (dict): Dictionary containing model names and model objects.\n",
    "        save_dir (str): Directory to save the models and parameters.\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        # Extract model parameters\n",
    "        if isinstance(model, TransformerSeq2Seq):\n",
    "            params = {\n",
    "                \"model_type\": \"Transformer\",\n",
    "                \"input_dim\": model.input_dim,\n",
    "                \"output_dim\": model.output_dim,\n",
    "                \"embed_dim\": model.fc_out.in_features, # model.embed_dim\n",
    "                \"n_heads\": model.n_heads,\n",
    "                \"num_layers\": model.num_layers,\n",
    "                \"dropout\": model.dropout\n",
    "            }\n",
    "\n",
    "        elif isinstance(model, CNNSeq2Seq):\n",
    "            params = {\n",
    "                \"model_type\": \"CNN\",\n",
    "                \"input_dim\": model.input_dim,\n",
    "                \"output_dim\": model.output_dim,\n",
    "                \"embed_dim\": model.embed_dim,\n",
    "                \"kernel_size\": model.kernel_size,\n",
    "                \"num_channels\": model.num_channels\n",
    "            }\n",
    "        elif isinstance(model, GRUSeq2Seq):\n",
    "            # input_dim, output_dim, embed_dim, hidden_dim, n_layers, dropout\n",
    "            params = {\n",
    "                \"model_type\": \"GRU\",\n",
    "                \"input_dim\": model.input_dim,\n",
    "                \"output_dim\": model.output_dim,\n",
    "                \"embed_dim\": model.embed_dim,\n",
    "                \"hidden_dim\": model.hidden_dim,\n",
    "                \"n_layers\": model.n_layers,\n",
    "                \"dropout\": model.dropout\n",
    "            }\n",
    "\n",
    "        elif isinstance(model, LSTMSeq2Seq):\n",
    "            params = {\n",
    "                \"model_type\": \"LSTM\",\n",
    "                \"input_dim\": model.input_dim,\n",
    "                \"output_dim\": model.output_dim,\n",
    "                \"embed_dim\": model.embed_dim,\n",
    "                \"hidden_dim\": model.hidden_dim,\n",
    "                \"n_layers\": model.n_layers,\n",
    "                \"dropout\": model.dropout\n",
    "            }\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type for {name}\")\n",
    "\n",
    "        # Save parameters as JSON\n",
    "        params_path = os.path.join(save_dir, f\"{name}_params.json\")\n",
    "        with open(params_path, 'w') as f:\n",
    "            json.dump(params, f)\n",
    "\n",
    "        # Save model weights\n",
    "        weights_path = os.path.join(save_dir, f\"{name}_weights.pt\")\n",
    "        torch.save(model.state_dict(), weights_path)\n",
    "\n",
    "        print(f\"Saved {name} to {params_path} and {weights_path}.\")\n",
    "\n",
    "# Function to load model parameters\n",
    "def load_model_params(params_path):\n",
    "    \"\"\"\n",
    "    Load model parameters from a JSON file.\n",
    "    Args:\n",
    "        params_path (str): Path to the JSON file containing model parameters.\n",
    "    Returns:\n",
    "        dict: Dictionary of model parameters.\n",
    "    \"\"\"\n",
    "    with open(params_path, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Function to load a model dynamically based on its parameters\n",
    "def load_model(model_name, save_dir, device):\n",
    "    \"\"\"\n",
    "    Dynamically load a model based on its type and saved parameters.\n",
    "    Args:\n",
    "        model_name (str): Name of the model to load.\n",
    "        save_dir (str): Directory where the model files are stored.\n",
    "        device (torch.device): Device to load the model onto.\n",
    "    Returns:\n",
    "        nn.Module: The loaded model.\n",
    "    \"\"\"\n",
    "    # Paths to parameters and weights\n",
    "    params_path = os.path.join(save_dir, f\"{model_name}_params.json\")\n",
    "    weights_path = os.path.join(save_dir, f\"{model_name}_weights.pt\")\n",
    "\n",
    "    # Load parameters\n",
    "    params = load_model_params(params_path)\n",
    "\n",
    "    # Reconstruct the model based on its type\n",
    "    if params[\"model_type\"] == \"Transformer\":\n",
    "        model = TransformerSeq2Seq(\n",
    "            params[\"input_dim\"],\n",
    "            params[\"output_dim\"],\n",
    "            params[\"embed_dim\"],\n",
    "            params[\"n_heads\"],\n",
    "            params[\"num_layers\"],\n",
    "            params[\"dropout\"]\n",
    "        )\n",
    "    elif params[\"model_type\"] == \"GRU\":\n",
    "        model = GRUSeq2Seq(\n",
    "            params[\"input_dim\"],\n",
    "            params[\"output_dim\"],\n",
    "            params[\"embed_dim\"],\n",
    "            params[\"hidden_dim\"],\n",
    "            params[\"n_layers\"],\n",
    "            params[\"dropout\"]\n",
    "        )\n",
    "\n",
    "    elif params[\"model_type\"] == \"CNN\":\n",
    "        model = CNNSeq2Seq(\n",
    "            params[\"input_dim\"],\n",
    "            params[\"output_dim\"],\n",
    "            params[\"embed_dim\"],\n",
    "            params[\"kernel_size\"],\n",
    "            params[\"num_channels\"]\n",
    "        )\n",
    "    elif params[\"model_type\"] == \"LSTM\":\n",
    "        model = LSTMSeq2Seq(\n",
    "            params[\"input_dim\"],\n",
    "            params[\"output_dim\"],\n",
    "            params[\"embed_dim\"],\n",
    "            params[\"hidden_dim\"],\n",
    "            params[\"n_layers\"],\n",
    "            params[\"dropout\"]\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model type: {params['model_type']}\")\n",
    "\n",
    "    # Load weights into the model\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=device))\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"Loaded {model_name} ({params['model_type']}) from {weights_path}.\")\n",
    "    return model\n",
    "\n",
    "# Save models and parameters\n",
    "save_dir = \"saved\"\n",
    "save_model_and_params(models, save_dir)\n",
    "\n",
    "# Load a specific model dynamically\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = \"LSTM_Seq2Seq\"  # Example: Change to \"Transformer_Seq2Seq\" or \"CNN_Seq2Seq\"\n",
    "loaded_model = load_model(model_name, save_dir, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, device):\n",
    "    \"\"\"\n",
    "    Translates a sentence using the given model.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence to translate.\n",
    "        model (nn.Module): The trained translation model with attributes:\n",
    "            - src_vocab (dict): Source vocabulary.\n",
    "            - tgt_vocab (dict): Target vocabulary.\n",
    "            - device (torch.device): Device to run the model on.\n",
    "            - max_len (int): Maximum sequence length for translation.\n",
    "\n",
    "    Returns:\n",
    "        str: Translated sentence.\n",
    "    \"\"\"\n",
    "    # Ensure the model is in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # load in vocb\n",
    "    src_vocab = load_vocab('saved/eng_vocab.json')\n",
    "    tgt_vocab = load_vocab('saved/jpn_vocab.json')\n",
    "\n",
    "    # Access vocabularies, device, and max_len from the model\n",
    "    max_len = MAX_LEN\n",
    "\n",
    "    # Tokenize the input sentence\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # Convert tokens to indices\n",
    "    src_indices = [src_vocab.get(token, src_vocab['<UNKNOWN>']) for token in tokens]\n",
    "    src_indices = [src_vocab['<START>']] + src_indices + [src_vocab['<END>']]\n",
    "\n",
    "    # Convert to tensor and add batch dimension\n",
    "    src_tensor = torch.tensor([src_indices], dtype=torch.long, device=device)\n",
    "\n",
    "    # Prepare initial target input (start with <START> token)\n",
    "    tgt_indices = [tgt_vocab['<START>']]\n",
    "    tgt_tensor = torch.tensor([tgt_indices], dtype=torch.long, device=device)\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        # Generate predictions\n",
    "        with torch.no_grad():\n",
    "            output = model(src_tensor, tgt_tensor)\n",
    "\n",
    "        # Get the next token (argmax of the last output step)\n",
    "        next_token = output[:, -1, :].argmax(dim=-1).item()\n",
    "\n",
    "        # Stop if <END> token is predicted\n",
    "        if next_token == tgt_vocab['<END>']:\n",
    "            break\n",
    "\n",
    "        # Add the predicted token to the target tensor\n",
    "        tgt_indices.append(next_token)\n",
    "        tgt_tensor = torch.tensor([tgt_indices], dtype=torch.long, device=device)\n",
    "\n",
    "    # Convert target indices to tokens\n",
    "    tgt_vocab_inv = {idx: token for token, idx in tgt_vocab.items()}\n",
    "    translated_tokens = [tgt_vocab_inv[idx] for idx in tgt_indices[1:]]  # Skip the <START> token\n",
    "    return ' '.join(translated_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CNN_Seq2Seq (CNN) from saved/CNN_Seq2Seq_weights.pt.\n"
     ]
    }
   ],
   "source": [
    "# Load a specific model dynamically\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model_name = \"CNN_Seq2Seq\" \n",
    "loaded_model = load_model(model_name, save_dir, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'出 出 出 出 こんなに 植物 植物 植物 植物 植物 植物 植物 植物 植物 植物'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = 'How are you?'\n",
    "translate_sentence(sentence, loaded_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "test_data = pd.read_csv('data_jesc/test', sep = '\\t')\n",
    "train_data = pd.read_csv('data_jesc/train', sep = '\\t')\n",
    "\n",
    "# adjust columns\n",
    "test_data.columns = ['ENG', 'JPN']\n",
    "train_data.columns = ['ENG', 'JPN']\n",
    "\n",
    "# Tokenize both English and Japanese sentences\n",
    "test_data['ENG'] = test_data['ENG'].apply(word_tokenize)\n",
    "test_data['JPN'] = test_data['JPN'].apply(word_tokenize)\n",
    "\n",
    "train_data['ENG'] = train_data['ENG'].apply(word_tokenize)\n",
    "train_data['JPN'] = train_data['JPN'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define special tokens\n",
    "SPECIAL_TOKENS = {'<PAD>': 0, '<START>': 1, '<END>': 2, '<UNKNOWN>': 3}\n",
    "\n",
    "# Build vocabulary\n",
    "def build_vocab(tokenized_data, special_tokens=SPECIAL_TOKENS):\n",
    "    MIN_FREQ = 5  # Minimum frequency for a word to be included in the vocabulary\n",
    "    vocab_counter = Counter(chain(*tokenized_data))\n",
    "    pruned_vocab = {k: v for k, v in vocab_counter.items() if v >= MIN_FREQ}\n",
    "    vocab = {word: idx + len(special_tokens) for idx, (word, _) in enumerate(pruned_vocab.items())}\n",
    "    vocab.update(special_tokens)\n",
    "    return vocab\n",
    "\n",
    "# Function to convert tokens to indices\n",
    "def tokens_to_indices(tokens, vocab, sos_eos=True):\n",
    "    indices = [vocab.get(token, vocab['<UNKNOWN>']) for token in tokens]\n",
    "    if sos_eos:\n",
    "        indices = [vocab['<START>']] + indices + [vocab['<END>']]\n",
    "    return indices\n",
    "\n",
    "# Convert tokens to indices and ensure indices are within range\n",
    "def safe_tokens_to_indices(tokens, vocab, sos_eos=True):\n",
    "    indices = [vocab.get(token, vocab['<UNKNOWN>']) for token in tokens]\n",
    "    if sos_eos:\n",
    "        indices = [vocab['<START>']] + indices + [vocab['<END>']]\n",
    "    return indices\n",
    "\n",
    "# Pad sequence to a fixed length\n",
    "MAX_LEN = 15\n",
    "def pad_sequence(sequence, max_len=MAX_LEN, pad_value=SPECIAL_TOKENS['<PAD>']):\n",
    "    return sequence[:max_len] + [pad_value] * max(0, max_len - len(sequence))\n",
    "\n",
    "# Apply the above functions to preprocess the data\n",
    "def preprocess_data(data, vocab, max_len=MAX_LEN):\n",
    "    return data.apply(lambda x: pad_sequence(safe_tokens_to_indices(x, vocab)))\n",
    "\n",
    "# Build vocabularies for English and Japanese\n",
    "eng_vocab = build_vocab(train_data['ENG'])\n",
    "jpn_vocab = build_vocab(train_data['JPN'])\n",
    "\n",
    "# Preprocess train and test data\n",
    "train_data['ENG'] = preprocess_data(train_data['ENG'], eng_vocab)\n",
    "train_data['JPN'] = preprocess_data(train_data['JPN'], jpn_vocab)\n",
    "\n",
    "test_data['ENG'] = preprocess_data(test_data['ENG'], eng_vocab)\n",
    "test_data['JPN'] = preprocess_data(test_data['JPN'], jpn_vocab)\n",
    "\n",
    "# Check language vocabulary size\n",
    "eng_words = len(eng_vocab)\n",
    "jpn_words = len(jpn_vocab)\n",
    "print(\"English vocabulary has {} unique words\".format(eng_words))\n",
    "print(\"Japanese vocabulary has {} unique words\".format(jpn_words))\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, src_data, tgt_data):\n",
    "        self.src_data = src_data\n",
    "        self.tgt_data = tgt_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.src_data[idx], dtype=torch.long), torch.tensor(self.tgt_data[idx], dtype=torch.long)\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = TranslationDataset(train_data['ENG'].tolist(), train_data['JPN'].tolist())\n",
    "test_dataset = TranslationDataset(test_data['ENG'].tolist(), test_data['JPN'].tolist())\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Verify a batch from DataLoader\n",
    "for src, tgt in train_loader:\n",
    "    print(\"Source batch shape:\", src.shape)\n",
    "    print(\"Target batch shape:\", tgt.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and Evaluation\n",
    "def train_model(model, train_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        output = output.reshape(-1, output.shape[-1])\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions, actuals = [], []\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in test_loader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt[:, :-1])\n",
    "            output = output.argmax(dim=-1)\n",
    "            predictions.extend(output.cpu().tolist())\n",
    "            actuals.extend(tgt[:, 1:].cpu().tolist())\n",
    "    return accuracy_score(actuals, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MODELS ###\n",
    "\n",
    "# Seq2Seq Models\n",
    "# LSTM Seq2Seq\n",
    "class LSTMSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim, hidden_dim, n_layers, dropout):\n",
    "        super(LSTMSeq2Seq, self).__init__()\n",
    "        self.embedding_src = nn.Embedding(input_dim, embed_dim)  # Source language embedding\n",
    "        self.embedding_tgt = nn.Embedding(output_dim, embed_dim)  # Target language embedding\n",
    "        self.encoder = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.decoder = nn.LSTM(embed_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)  # Final output layer\n",
    "\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.embedding(src)\n",
    "        _, (hidden, cell) = self.encoder(src_embedded)\n",
    "        tgt_embedded = self.embedding(tgt)\n",
    "        outputs, _ = self.decoder(tgt_embedded, (hidden, cell))\n",
    "        return self.fc_out(outputs)\n",
    "\n",
    "# Transformer Seq2Seq\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim, n_heads, num_layers, dropout):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "        self.embedding_src = nn.Embedding(input_dim, embed_dim)\n",
    "        self.embedding_tgt = nn.Embedding(output_dim, embed_dim)\n",
    "        self.transformer = nn.Transformer(embed_dim, n_heads, num_layers, num_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.embedding_src(src).permute(1, 0, 2)  # (seq_len, batch, embed_dim)\n",
    "        tgt_embedded = self.embedding_tgt(tgt).permute(1, 0, 2)\n",
    "        output = self.transformer(src_embedded, tgt_embedded)\n",
    "        return self.fc_out(output.permute(1, 0, 2))\n",
    "\n",
    "# CNN Seq2Seq\n",
    "class CNNSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim, kernel_size, num_channels):\n",
    "        super(CNNSeq2Seq, self).__init__()\n",
    "        self.embedding_src = nn.Embedding(input_dim, embed_dim)\n",
    "        self.embedding_tgt = nn.Embedding(output_dim, embed_dim)\n",
    "        self.encoder = nn.Conv1d(embed_dim, num_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.decoder = nn.Conv1d(num_channels + embed_dim, num_channels, kernel_size, padding=kernel_size // 2)\n",
    "        self.fc_out = nn.Linear(num_channels, output_dim)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_embedded = self.embedding_src(src).permute(0, 2, 1)\n",
    "        encoder_outputs = self.encoder(src_embedded)\n",
    "        tgt_embedded = self.embedding_tgt(tgt).permute(0, 2, 1)\n",
    "        decoder_inputs = torch.cat((encoder_outputs, tgt_embedded), dim=1)\n",
    "        decoder_outputs = self.decoder(decoder_inputs).permute(0, 2, 1)\n",
    "        return self.fc_out(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Max index in train_data['ENG']:\", max(chain(*train_data['ENG'])))\n",
    "print(\"Max index in train_data['JPN']:\", max(chain(*train_data['JPN'])))\n",
    "print(\"Max index in test_data['ENG']:\", max(chain(*test_data['ENG'])))\n",
    "print(\"Max index in test_data['JPN']:\", max(chain(*test_data['JPN'])))\n",
    "print(\"English vocab size:\", len(eng_vocab))\n",
    "print(\"Japanese vocab size:\", len(jpn_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Print a batch from the DataLoader\n",
    "for src, tgt in train_loader:\n",
    "    print(\"Source batch sample:\", src[0])\n",
    "    print(\"Target batch sample:\", tgt[0])\n",
    "    print(\"Max source index in batch:\", src.max().item())\n",
    "    print(\"Max target index in batch:\", tgt.max().item())\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Compare models\n",
    "models = {\n",
    "    'LSTM_Seq2Seq': LSTMSeq2Seq(len(eng_vocab), len(jpn_vocab), 100, 100, 2, 0.5),\n",
    "    'Transformer_Seq2Seq': TransformerSeq2Seq(len(eng_vocab), len(jpn_vocab), 100, 100, 2, 0.5),\n",
    "    'CNN_Seq2Seq': CNNSeq2Seq(len(eng_vocab), len(jpn_vocab), 100, 2, 100)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=SPECIAL_TOKENS['<PAD>'])\n",
    "    print(f\"Training {name}...\")\n",
    "    for epoch in range(10):\n",
    "        train_loss = train_model(model, train_loader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch + 1}, Train Loss: {train_loss:.4f}\")\n",
    "    test_loss = evaluate_model(model, test_loader, criterion, device)\n",
    "    results[name] = test_loss\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "for model_name, loss in results.items():\n",
    "    print(f\"{model_name}: Test Loss = {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data['ENG'].head())\n",
    "print(train_data['JPN'].head())\n",
    "print(type(train_data['ENG'].iloc[0]))\n",
    "print(type(train_data['JPN'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Dataset class\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, eng_data, jpn_data):\n",
    "        self.eng_data = torch.tensor(eng_data, dtype=torch.long)\n",
    "        self.jpn_data = torch.tensor(jpn_data, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.eng_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.eng_data[idx], self.jpn_data[idx]\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, hidden_dim, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, tgt, hidden, cell):\n",
    "        tgt = tgt.unsqueeze(1)  # Add time dimension\n",
    "        embedded = self.embedding(tgt)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "# Seq2Seq Model\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # First input to the decoder is the <START> token\n",
    "        input_token = tgt[:, 0]\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input_token, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            # Use teacher forcing\n",
    "            top1 = output.argmax(1)\n",
    "            input_token = tgt[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class TransformerSeq2Seq(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, embed_dim, num_heads, num_layers, dropout):\n",
    "        super(TransformerSeq2Seq, self).__init__()\n",
    "        self.embedding_src = nn.Embedding(input_dim, embed_dim)\n",
    "        self.embedding_tgt = nn.Embedding(output_dim, embed_dim)\n",
    "        self.positional_encoding = self._generate_positional_encoding(embed_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc_out = nn.Linear(embed_dim, output_dim)\n",
    "\n",
    "    def _generate_positional_encoding(self, embed_dim, max_len=5000):\n",
    "        pos_encoding = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\n",
    "        pos_encoding[:, 0::2] = torch.sin(position * div_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(position * div_term)\n",
    "        return pos_encoding.unsqueeze(0)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src_seq_len = src.size(1)\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "\n",
    "        src = self.embedding_src(src) + self.positional_encoding[:, :src_seq_len, :]\n",
    "        tgt = self.embedding_tgt(tgt) + self.positional_encoding[:, :tgt_seq_len, :]\n",
    "\n",
    "        src = src.permute(1, 0, 2)  # (seq_len, batch, embed_dim)\n",
    "        tgt = tgt.permute(1, 0, 2)  # (seq_len, batch, embed_dim)\n",
    "\n",
    "        outputs = self.transformer(src, tgt)\n",
    "        outputs = outputs.permute(1, 0, 2)  # (batch, seq_len, embed_dim)\n",
    "\n",
    "        return self.fc_out(outputs)\n",
    "\n",
    "# Example Usage\n",
    "INPUT_DIM = 1000\n",
    "OUTPUT_DIM = 1000\n",
    "EMBED_DIM = 256\n",
    "NUM_HEADS = 8\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = TransformerSeq2Seq(INPUT_DIM, OUTPUT_DIM, EMBED_DIM, NUM_HEADS, NUM_LAYERS, DROPOUT)\n",
    "\n",
    "src = torch.randint(0, INPUT_DIM, (32, 10))  # (batch, src_len)\n",
    "tgt = torch.randint(0, OUTPUT_DIM, (32, 10))  # (batch, tgt_len)\n",
    "\n",
    "outputs = model(src, tgt)\n",
    "print(outputs.shape)  # (batch, tgt_len, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, kernel_size, num_channels):\n",
    "        super(CNNEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=embed_dim,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=(kernel_size - 1) // 2\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src).permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
    "        conv_out = self.conv(embedded)\n",
    "        return self.relu(conv_out.permute(0, 2, 1))  # (batch, seq_len, num_channels)\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self, output_dim, embed_dim, kernel_size, num_channels):\n",
    "        super(CNNDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embed_dim)\n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels=num_channels + embed_dim,\n",
    "            out_channels=num_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            padding=(kernel_size - 1) // 2\n",
    "        )\n",
    "        self.fc = nn.Linear(num_channels, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, tgt, encoder_outputs):\n",
    "        embedded = self.embedding(tgt).permute(0, 2, 1)  # (batch, embed_dim, seq_len)\n",
    "        decoder_input = torch.cat([encoder_outputs.permute(0, 2, 1), embedded], dim=1)\n",
    "        conv_out = self.conv(decoder_input)\n",
    "        conv_out = self.relu(conv_out.permute(0, 2, 1))  # (batch, seq_len, num_channels)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "class Seq2SeqCNN(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2SeqCNN, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        encoder_outputs = self.encoder(src)\n",
    "        outputs = self.decoder(tgt, encoder_outputs)\n",
    "        return outputs\n",
    "\n",
    "# Example Usage\n",
    "INPUT_DIM = 1000\n",
    "OUTPUT_DIM = 1000\n",
    "EMBED_DIM = 256\n",
    "KERNEL_SIZE = 3\n",
    "NUM_CHANNELS = 256\n",
    "\n",
    "encoder = CNNEncoder(INPUT_DIM, EMBED_DIM, KERNEL_SIZE, NUM_CHANNELS)\n",
    "decoder = CNNDecoder(OUTPUT_DIM, EMBED_DIM, KERNEL_SIZE, NUM_CHANNELS)\n",
    "model = Seq2SeqCNN(encoder, decoder)\n",
    "\n",
    "src = torch.randint(0, INPUT_DIM, (32, 10))  # (batch, src_len)\n",
    "tgt = torch.randint(0, OUTPUT_DIM, (32, 10))  # (batch, tgt_len)\n",
    "\n",
    "outputs = model(src, tgt)\n",
    "print(outputs.shape)  # (batch, tgt_len, output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = len(eng_vocab)\n",
    "OUTPUT_DIM = len(jpn_vocab)\n",
    "EMBED_DIM = 100 #256\n",
    "HIDDEN_DIM = 100 #512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "LEARNING_RATE = 1e-3\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Initialize encoder, decoder, and model\n",
    "encoder = Encoder(INPUT_DIM, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=SPECIAL_TOKENS['<PAD>'])\n",
    "\n",
    "# Prepare dataset and dataloader\n",
    "train_dataset = TranslationDataset(train_data['ENG'], train_data['JPN'])\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f'Running {epoch+1}')\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for src, tgt in train_loader:\n",
    "        src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "\n",
    "        # Reshape outputs and targets\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{EPOCHS}, Loss: {epoch_loss/len(train_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(DEVICE), tgt.to(DEVICE)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=SPECIAL_TOKENS['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        embedded = self.embedding(src)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim, padding_idx=SPECIAL_TOKENS['<PAD>'])\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1)  # Add batch dimension\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))\n",
    "        return prediction, hidden, cell\n",
    "    \n",
    "### MODEL ###\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        tgt_vocab_size = self.decoder.embedding.num_embeddings\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = tgt[:, 0]  # <SOS>\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            top1 = output.argmax(1)\n",
    "            input = tgt[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "### END OF MODEL ###\n",
    "\n",
    "def train(model, dataloader, optimizer, criterion, device, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for src, tgt in dataloader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = model(src, tgt)\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        # Remove <SOS> token for target\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def test(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0)  # No teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.src = torch.tensor(data['ENG'].tolist(), dtype=torch.long)\n",
    "        self.tgt = torch.tensor(data['JPN'].tolist(), dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.src[idx], self.tgt[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "INPUT_DIM = len(eng_vocab)\n",
    "OUTPUT_DIM = len(jpn_vocab)\n",
    "EMBEDDING_DIM = 20 #50\n",
    "HIDDEN_DIM = 16\n",
    "N_LAYERS = 2\n",
    "DROPOUT = 0.5\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = DEVICE\n",
    "N_EPOCHS = 5\n",
    "CLIP = 1\n",
    "PAD_IDX = SPECIAL_TOKENS['<PAD>']\n",
    "\n",
    "# init model\n",
    "encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, N_LAYERS, DROPOUT)\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.AdamW(model.parameters())\n",
    "\n",
    "# load data into a dataloader\n",
    "train_dataset = TranslationDataset(train_data)\n",
    "dev_dataset = TranslationDataset(dev_data)\n",
    "'''train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32)'''\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "# training #\n",
    "for epoch in range(N_EPOCHS):\n",
    "    print(epoch)\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device, CLIP)\n",
    "    dev_loss = test(model, dev_loader, criterion, device)\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02}')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {torch.exp(torch.tensor(train_loss)):.2f}')\n",
    "    print(f'\\tVal Loss: {dev_loss:.3f} | Val PPL: {torch.exp(torch.tensor(dev_loss)):.2f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "torch.save(model.state_dict(), 'seq2seq_model.pt')\n",
    "\n",
    "# Load model\n",
    "model.load_state_dict(torch.load('seq2seq_model.pt'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(model, sentence, eng_vocab, jpn_idx2word, max_len=MAX_LEN):\n",
    "    model.eval()\n",
    "    tokens = [eng_vocab.get(word, eng_vocab['<UNKNOWN>']) for word in word_tokenize(sentence)]\n",
    "    src_tensor = torch.tensor([tokens + [PAD_IDX] * (max_len - len(tokens))], dtype=torch.long).to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tensor)\n",
    "        input = torch.tensor([SPECIAL_TOKENS['<START>']], dtype=torch.long).to(DEVICE)\n",
    "        translated_sentence = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = model.decoder(input, hidden, cell)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == SPECIAL_TOKENS['<END>']:\n",
    "                break\n",
    "            translated_sentence.append(jpn_idx2word[top1.item()])\n",
    "            input = top1\n",
    "\n",
    "    return ' '.join(translated_sentence)\n",
    "\n",
    "# Example\n",
    "sentence = \"The cat runs, the dog barks\"\n",
    "print(translate_sentence(model, sentence, eng_vocab, jpn_idx2word))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
